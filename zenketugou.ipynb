{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bc7f0b4-0c96-4ba2-a60a-84e127d20afb",
   "metadata": {},
   "source": [
    "# データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47bb101b-256c-4487-9179-d7d67c652c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "# 自作モデルのインポート\n",
    "import models.ore_no_zenketugou_model as zenketu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "511b8735-8929-48b6-9b1b-2a8e08a43d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# cifar100を利用する\n",
    "data_path = './cifar-100'\n",
    "# 訓練データと検証データのダウンロード\n",
    "tensor_cifar100_train = datasets.CIFAR100(data_path,\n",
    "                                          train=True,\n",
    "                                          download=True,\n",
    "                                          transform=transforms.ToTensor())\n",
    "tensor_cifar100_val = datasets.CIFAR100(data_path,\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac8644b-be19-4f9b-a2e3-b8dd74a1baf3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cifar100の上位クラスのラベル\n",
    "super_class_names = [\n",
    "  'aquatic mammals',  # 0：水生哺乳類\n",
    "  'fish',  # 1：魚\n",
    "  'flowers',  # ：花\n",
    "  'food containers',  # 3：食品容器\n",
    "  'fruit and vegetables',  # 4：果物と野菜\n",
    "  'household electrical devices',  # 5：家電\n",
    "  'household furniture',  # 6：家具\n",
    "  'insects',  # 7：昆虫\n",
    "  'large carnivores',  # 8：大型の肉食動物\n",
    "  'large man-made outdoor things',  # 9：大型の建造物\n",
    "  'large natural outdoor scenes',  # 10：大自然の風景\n",
    "  'large omnivores and herbivores',  # 11：大型の雑食動物と草食動物\n",
    "  'medium-sized mammals',  # 12：中型の哺乳類\n",
    "  'non-insect invertebrates',  # 13：昆虫ではない無脊椎動物\n",
    "  'people',  # 14：人\n",
    "  'reptiles',  # 15：爬虫類\n",
    "  'small mammals',  # 16：小型の哺乳類\n",
    "  'trees',  # 17：木\n",
    "  'vehicles 1',  # 18：車両1\n",
    "  'vehicles 2',  # 19：車両2\n",
    "]\n",
    "\n",
    "# cifar100の各ラベル\n",
    "class_names = [\n",
    "  'apples',  # 0：りんご\n",
    "  'aquarium fish',  # 1：観賞魚\n",
    "  'baby',  # 2：赤ちゃん\n",
    "  'bear',  # 3：クマ\n",
    "  'beaver',  # 4：ビーバー\n",
    "  'bed',  # 5：ベッド\n",
    "  'bee',  # 6：蜂\n",
    "  'beetle',  # 7：カブトムシ\n",
    "  'bicycle',  # 8：自転車\n",
    "  'bottles',  # 9：ボトル\n",
    "  'bowls',  # 10：ボウル\n",
    "  'boy',  # 11：少年\n",
    "  'bridge',  # 12：橋\n",
    "  'bus',  # 13：バス\n",
    "  'butterfly',  # 14：蝶\n",
    "  'camel',  # 15：ラクダ\n",
    "  'cans',  # 16：缶\n",
    "  'castle',  # 17：城\n",
    "  'caterpillar',  # 18：毛虫\n",
    "  'cattle',  # 19：牛\n",
    "  'chair',  # 20：椅子\n",
    "  'chimpanzee',  # 21：チンパンジー\n",
    "  'clock',  # 22：時計\n",
    "  'cloud',  # 23：雲\n",
    "  'cockroach',  # 24：ゴキブリ\n",
    "  'couch',  # 25：ソファー\n",
    "  'crab',  # 26：カニ\n",
    "  'crocodile',  # 27：ワニ\n",
    "  'cups',  # 28：カップ\n",
    "  'dinosaur',  # 29：恐竜\n",
    "  'dolphin',  # 30：イルカ\n",
    "  'elephant',  # 31：象\n",
    "  'flatfish',  # 32：ヒラメ\n",
    "  'forest',  # 33：森\n",
    "  'fox',  # 34：キツネ\n",
    "  'girl',  # 35：少女\n",
    "  'hamster',  # 36：ハムスター\n",
    "  'house',  # 37：家\n",
    "  'kangaroo',  # 38：カンガルー\n",
    "  'computer keyboard',  # 39：コンピューターのキーボード\n",
    "  'lamp',  # 40：ランプ\n",
    "  'lawn-mower',  # 41：芝刈り機\n",
    "  'leopard',  # 42：ヒョウ\n",
    "  'lion',  # 43：ライオン\n",
    "  'lizard',  # 44：トカゲ\n",
    "  'lobster',  # 45：ロブスター\n",
    "  'man',  # 46：成人男性\n",
    "  'maple',  # 47：もみじ\n",
    "  'motorcycle',  # 48：オートバイ\n",
    "  'mountain',  # 49：山\n",
    "  'mouse',  # 50：ねずみ\n",
    "  'mushrooms',  # 51：きのこ\n",
    "  'oak',  # 52：オーク\n",
    "  'oranges',  # 53：オレンジ\n",
    "  'orchids',  # 54：蘭\n",
    "  'otter',  # 55：カワウソ\n",
    "  'palm',  # 56：ヤシ\n",
    "  'pears',  # 57：洋ナシ\n",
    "  'pickup truck',  # 58：ピックアップトラック\n",
    "  'pine',  # 59：松\n",
    "  'plain',  # 60：平野\n",
    "  'plates',  # 61：皿\n",
    "  'poppies',  # 62：ポピー\n",
    "  'porcupine',  # 63：ヤマアラシ\n",
    "  'possum',  # 64：フクロネズミ\n",
    "  'rabbit',  # 65：ウサギ\n",
    "  'raccoon',  # 66：アライグマ\n",
    "  'ray',  # 67：エイ\n",
    "  'road',  # 68：道路\n",
    "  'rocket',  # 69：ロケット\n",
    "  'roses',  # 70：バラ\n",
    "  'sea',  # 71：海\n",
    "  'seal',  # 72：アザラシ\n",
    "  'shark',  # 73：サメ\n",
    "  'shrew',  # 74：トガリネズミ\n",
    "  'skunk',  # 75：スカンク\n",
    "  'skyscraper',  # 76：超高層ビル\n",
    "  'snail',  # 77：カタツムリ\n",
    "  'snake',  # 78：ヘビ\n",
    "  'spider',  # 79：クモ\n",
    "  'squirrel',  # 80：リス\n",
    "  'streetcar',  # 81：路面電車\n",
    "  'sunflowers',  # 82：ひまわり\n",
    "  'sweet peppers',  # 83：パプリカ\n",
    "  'table',  # 84：テーブル\n",
    "  'tank',  # 85：タンク\n",
    "  'telephone',  # 86：電話\n",
    "  'television',  # 87：テレビ\n",
    "  'tiger',  # 88：トラ\n",
    "  'tractor',  # 89：トラクター\n",
    "  'train',  # 90：電車\n",
    "  'trout',  # 91：マス\n",
    "  'tulips',  # 92：チューリップ\n",
    "  'turtle',  # 93：カメ\n",
    "  'wardrobe',  # 94：ワードローブ\n",
    "  'whale',  # 95：クジラ\n",
    "  'willow',  # 96：柳\n",
    "  'wolf',  # 97：オオカミ\n",
    "  'woman',  # 98：成人女性\n",
    "  'worm',  # 99：ミミズ\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50db966-3445-4ab6-8936-b1f87cbcf13a",
   "metadata": {},
   "source": [
    "## 画像の標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83998dfa-f56e-45e5-a9b7-0dd3ecae9a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32, 50000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テンソルを束ねる\n",
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar100_train], dim = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14b54ba4-dfde-41ad-8d4d-c8d887a29b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5071, 0.4865, 0.4409]), tensor([0.2673, 0.2564, 0.2762]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RGB各々の平均、標準偏差を計算する\n",
    "means = imgs.view(3,-1).mean(dim=1)\n",
    "stds = imgs.view(3,-1).std(dim=1)\n",
    "means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7d67d7-1ab4-4a56-8321-b16275d8174c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 各画像データを標準化する\n",
    "transformed_tensor_cifar100_train = datasets.CIFAR100(data_path,\n",
    "                                                      train=True,\n",
    "                                                      download=False,\n",
    "                                                      transform=transforms.Compose([\n",
    "                                                          transforms.ToTensor(),\n",
    "                                                          transforms.Normalize(means,stds)\n",
    "                                                      ]))\n",
    "transformed_tensor_cifar100_val = datasets.CIFAR100(data_path,\n",
    "                                                   train=False,\n",
    "                                                   download=False,\n",
    "                                                   transform = transforms.Compose([\n",
    "                                                       transforms.ToTensor(),\n",
    "                                                       transforms.Normalize(means,stds)\n",
    "                                                   ]))\n",
    "\n",
    "transformed_imgs = torch.stack([img_t for img_t, _ in transformed_tensor_cifar100_train], dim = 3)\n",
    "transformed_imgs_val = torch.stack([img_t for img_t, _ in transformed_tensor_cifar100_val], dim = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906799cd-d9c9-4a8d-86bb-45aaeb69a17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# macのGPUが使えるか確認\n",
    "device = (torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c335a2fc-fb53-45b0-8d36-6d46693c7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainデータとvalデータの正解率の確認\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    transformed_tensor_cifar100_train,\n",
    "    batch_size=100,\n",
    "    shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    transformed_tensor_cifar100_val,\n",
    "    batch_size=100,\n",
    "    shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]: # 最初にTrain、printで結果を表示後、valを計算して結果を表示する流れ\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad(): # 勾配を計算させないのための記述\n",
    "            for imgs, labels in loader:\n",
    "                model.to(device='mps') # モデルをGPU上に持ってくる\n",
    "                imgs = imgs.to(device='mps')\n",
    "                labels = labels.to(device='mps')\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "                # 最初にtrain、後にvalの正解率を出す。\n",
    "                if name == 'train':\n",
    "                    train_accuracy = correct / total\n",
    "                else:\n",
    "                    val_accuracy = correct / total\n",
    "    return train_accuracy, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94d43740-cf0e-4575-b9fe-90a59408401a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 訓練ループ\n",
    "def train_loop(n_epochs, optimizer, model, loss_fn, train, val):\n",
    "    loss_train_lists = np.array([])\n",
    "    loss_val_lists = np.array([])\n",
    "    train_accuracies = np.array([])\n",
    "    val_accuracies = np.array([])\n",
    "    \n",
    "    device = (torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu'))\n",
    "    # GPU上で訓練するのでモデルを移動。toを使う\n",
    "    model.to(device=device)\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        loss_train = 0.0 # エポックごとにtrainとvalの損失関数を計算する\n",
    "        loss_val = 0.0\n",
    "\n",
    "        for imgs, labels in train:\n",
    "            imgs = imgs.to(device=device) # 画像とラベルをGPU上へ送る\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs) # 訓練結果\n",
    "            loss = loss_fn(outputs,labels) # 損失の計算\n",
    "\n",
    "            optimizer.zero_grad() # 勾配の値を一旦初期化。勾配は累積される。エポック毎に累積されないようにする。\n",
    "            loss.backward() # バックプロぱゲーションで勾配を計算\n",
    "            optimizer.step() # 計算した勾配を基にパラメータを調整\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        for val_imgs, val_labels in val:\n",
    "            with torch.no_grad():\n",
    "                val_imgs = val_imgs.to(device=device)\n",
    "                val_labels = val_labels.to(device=device)\n",
    "                val_outputs = model(val_imgs)\n",
    "                val_loss = loss_fn(val_outputs, val_labels)\n",
    "            loss_val += val_loss.item()\n",
    "\n",
    "        # 損失\n",
    "        loss_train_lists = np.append(loss_train_lists, loss_train/len(train))\n",
    "        loss_val_lists = np.append(loss_val_lists, loss_val/len(val))\n",
    "        \n",
    "        # 正解率の計算\n",
    "        train_accuracy, val_accuracy = validate(model, train_loader, val_loader)\n",
    "        train_accuracies = np.append(train_accuracies, train_accuracy)\n",
    "        val_accuracies = np.append(val_accuracies, val_accuracy)\n",
    "        # model.to(device=device)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch in [1,2,3]:\n",
    "            print(f'{datetime.datetime.now()}Epoch:{epoch},Traing_loss:{loss_train/len(train)},val_loss{loss_val/len(val)}')\n",
    "        \n",
    "    return model, loss_train_lists, loss_val_lists, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57d450e5-2ad5-4ca1-b9cf-b514cd037983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1624676, [1572864, 512, 51200, 100])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルのインスタンスの作成\n",
    "model = zenketu.Zenketugou()\n",
    "# パラメータの数を確認\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21c2d89b-dd5f-4e73-87b7-d9d0eb77007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータの設定\n",
    "# データローダーに入れることで、簡単にデータを取り扱う\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    transformed_tensor_cifar100_train,\n",
    "    batch_size=100,\n",
    "    shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    transformed_tensor_cifar100_val,\n",
    "    batch_size=3,\n",
    "    shuffle=False)\n",
    "# 学習率の設定\n",
    "learning_rate = 1e-2\n",
    "# パラメータの修正に確率的勾配降下方を利用\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "# 損失関数を定義\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# エポック数を設定\n",
    "n_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0440b73-52bc-4a2c-8c11-5d32d9c1160e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# モデルの訓練\n",
    "# model, loss_train_lists, loss_val_lists, train_accuracies, val_accuracies = train_loop(n_epochs = n_epochs,\n",
    "#                                             optimizer=optimizer,\n",
    "#                                             model = model,\n",
    "#                                             loss_fn = loss_fn,\n",
    "#                                             train = train_loader,\n",
    "#                                             val = val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edf02b07-0ecd-4976-97ed-f36e58653a93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j0/dprt9y4n7hjcjnr0fw0wxf300000gn/T/ipykernel_16367/3151659384.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 損失ではなく正解率を出す\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'only_zenketugou_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "# 正解率を出す\n",
    "plt.plot(train_accuracies,label = 'train')\n",
    "plt.plot(val_accuracies, label ='val')\n",
    "plt.title('only_zenketugou_model')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "# plt.savefig('picture/accuracy_smple_zenketugou.png')\n",
    "plt.show()\n",
    "# 損失\n",
    "plt.plot(loss_train_lists,label = 'train')\n",
    "plt.plot(loss_val_lists, label ='val')\n",
    "plt.title('only_zenketugou_model')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "# plt.savefig('picture/loss_smple_zenketugou.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817109d3-6a5b-4c3f-acc6-4f7d1b5deca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのパラメータの保存保存\n",
    "# d_path = './para_models/'\n",
    "# torch.save(model.state_dict(), d_path + 'only_zenketugou_model2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29ddee-dd88-45e7-97cb-5d9c87cee1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの呼び出し\n",
    "# loaded_model = zenketu.Zenketugou()\n",
    "# loaded_model.load_state_dict(torch.load(d_path + 'only_zenketugou_model2.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd542d33-1d84-4b85-8715-b3aca83f3dba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 結果をdfに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4456f4f4-c579-4aa9-bde5-0197e94104e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>archi</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>loss_fn</th>\n",
       "      <th>regularization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023/01/27/17:40</td>\n",
       "      <td>ore_no_zenketugou</td>\n",
       "      <td>NN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>SDG</td>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>CrossEntropyLoss()</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date               name archi  train_accuracy  val_accuracy  \\\n",
       "0  2023/01/27/17:40  ore_no_zenketugou    NN             1.0           0.2   \n",
       "\n",
       "  optimizer  n_epochs  batch_size             loss_fn  regularization  \n",
       "0       SDG       500         100  CrossEntropyLoss()               0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データの読み込み\n",
    "results_df = pd.read_csv('results_df.csv',index_col=0)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4121b92a-c655-4784-b869-c894f18fc3a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j0/dprt9y4n7hjcjnr0fw0wxf300000gn/T/ipykernel_16367/3318842035.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m        'n_epochs','batch_size','loss_fn','regularization']\n\u001b[1;32m      4\u001b[0m add_df = pd.DataFrame([[datetime.datetime.now().strftime('%Y/%m/%d/%H:%M'),\n\u001b[0;32m----> 5\u001b[0;31m                         \u001b[0;34m'ore_no_zenketugou'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                         'SDG', n_epochs, 100, loss_fn, 0]],columns=col)\n\u001b[1;32m      7\u001b[0m \u001b[0madd_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "# 追加したい結果をまとめたdfを作成\n",
    "col = ['date','name','archi','train_accuracy','val_accuracy','optimizer',\n",
    "       'n_epochs','batch_size','loss_fn','regularization']\n",
    "add_df = pd.DataFrame([[datetime.datetime.now().strftime('%Y/%m/%d/%H:%M'),\n",
    "                        'ore_no_zenketugou', 'NN', train_accuracies[-1], val_accuracies[-1],\n",
    "                        'SDG', n_epochs, 100, loss_fn, 0]],columns=col)\n",
    "add_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba68e329-9ee0-467f-90bb-a9b5fd60bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df, add_df], axis=0)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb230d8-dba7-4610-b0b4-3b5e4d4c467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "results_df.to_csv('results_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804670ae-844a-43c7-9b72-299db9ed9987",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6ae6dce-c9a1-4b96-88c1-14534d6cd8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練ループ\n",
    "def train_loop(n_epochs, optimizer, model, loss_fn, train, val):\n",
    "    loss_train_lists = np.array([])\n",
    "    loss_val_lists = np.array([])\n",
    "    train_accuracies = np.array([])\n",
    "    val_accuracies = np.array([])\n",
    "    \n",
    "    device = (torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu'))\n",
    "    # GPU上で訓練するのでモデルを移動。toを使う\n",
    "    model.to(device=device)\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        loss_train = 0.0 # エポックごとにtrainとvalの損失関数を計算する\n",
    "        loss_val = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        for imgs, labels in train:\n",
    "            imgs = imgs.to(device=device) # 画像とラベルをGPU上へ送る\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs) # 訓練結果\n",
    "            loss = loss_fn(outputs,labels) # 損失の計算\n",
    "\n",
    "            optimizer.zero_grad() # 勾配の値を一旦初期化。勾配は累積される。エポック毎に累積されないようにする。\n",
    "            loss.backward() # バックプロぱゲーションで勾配を計算\n",
    "            optimizer.step() # 計算した勾配を基にパラメータを調整\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        for val_imgs, val_labels in val:\n",
    "            with torch.no_grad():\n",
    "                val_imgs = val_imgs.to(device=device)\n",
    "                val_labels = val_labels.to(device=device)\n",
    "                val_outputs = model(val_imgs)\n",
    "                val_loss = loss_fn(val_outputs, val_labels)\n",
    "            loss_val += val_loss.item()\n",
    "\n",
    "        loss_train_lists = np.append(loss_train_lists, loss_train/len(train))\n",
    "        loss_val_lists = np.append(loss_val_lists, loss_val/len(val))\n",
    "        \n",
    "        train_accuracy, val_accuracy = validate(model, train_loader, val_loader)\n",
    "        train_accuracies = np.append(train_accuracies, train_accuracy)\n",
    "        val_accuracies = np.append(val_accuracies, val_accuracy)\n",
    "        \n",
    "\n",
    "        if epoch % 10 == 0 or epoch in [1,2,3]:\n",
    "            print(f'{datetime.datetime.now()}Epoch:{epoch},Traing_loss:{loss_train/len(train)},val_loss{loss_val/len(val)}')\n",
    "        \n",
    "    return model, loss_train_lists, loss_val_lists, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0fc6b12-af78-41fd-925c-0ad4710c04b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23178340, [12582912, 4096, 8388608, 2048, 2097152, 1024, 102400, 100])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = zenketu.DropoutZenketugou()\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35652d51-aae1-4474-a65a-a1e9f617dd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-27 17:47:17.559403Epoch:1,Traing_loss:4.615701639175415,val_loss4.610987784266114\n",
      "2023-01-27 17:48:12.777012Epoch:2,Traing_loss:4.615298274993896,val_loss4.610987784266114\n",
      "2023-01-27 17:49:07.130571Epoch:3,Traing_loss:4.615978754997253,val_loss4.610987784266114\n",
      "2023-01-27 17:55:28.988073Epoch:10,Traing_loss:4.615024746894837,val_loss4.610987784266114\n",
      "2023-01-27 18:04:58.647342Epoch:20,Traing_loss:4.615918796539306,val_loss4.610987784266114\n",
      "2023-01-27 18:14:25.604645Epoch:30,Traing_loss:4.615610251426697,val_loss4.610987784266114\n",
      "2023-01-27 18:25:29.044545Epoch:40,Traing_loss:4.616480526924133,val_loss4.610987784266114\n",
      "2023-01-27 18:34:51.777375Epoch:50,Traing_loss:4.615583911895752,val_loss4.610987784266114\n",
      "2023-01-27 18:44:23.755254Epoch:60,Traing_loss:4.615286716461181,val_loss4.610987784266114\n",
      "2023-01-27 18:54:04.257887Epoch:70,Traing_loss:4.615837982177735,val_loss4.610987784266114\n",
      "2023-01-27 19:02:44.436264Epoch:80,Traing_loss:4.615783779144287,val_loss4.610987784266114\n",
      "2023-01-27 19:11:28.687858Epoch:90,Traing_loss:4.6147622356414795,val_loss4.610987784266114\n",
      "2023-01-27 19:20:10.564931Epoch:100,Traing_loss:4.615765634536743,val_loss4.610987784266114\n",
      "2023-01-27 19:28:55.802468Epoch:110,Traing_loss:4.614787919998169,val_loss4.610987784266114\n",
      "2023-01-27 19:37:39.536181Epoch:120,Traing_loss:4.61527478981018,val_loss4.610987784266114\n",
      "2023-01-27 19:46:20.038855Epoch:130,Traing_loss:4.614881316184998,val_loss4.610987784266114\n",
      "2023-01-27 19:55:03.583782Epoch:140,Traing_loss:4.6152955331802366,val_loss4.610987784266114\n",
      "2023-01-27 20:03:53.435900Epoch:150,Traing_loss:4.615784231185913,val_loss4.610987784266114\n",
      "2023-01-27 20:12:40.519331Epoch:160,Traing_loss:4.616385657310486,val_loss4.610987784266114\n",
      "2023-01-27 20:21:33.874684Epoch:170,Traing_loss:4.616267821311951,val_loss4.610987784266114\n",
      "2023-01-27 20:30:16.386042Epoch:180,Traing_loss:4.6165655326843265,val_loss4.610987784266114\n",
      "2023-01-27 20:38:59.770876Epoch:190,Traing_loss:4.616265100479126,val_loss4.610987784266114\n",
      "2023-01-27 20:47:43.399971Epoch:200,Traing_loss:4.615655608177185,val_loss4.610987784266114\n",
      "2023-01-27 20:56:17.113641Epoch:210,Traing_loss:4.615547436714173,val_loss4.610987784266114\n",
      "2023-01-27 21:05:00.215993Epoch:220,Traing_loss:4.616014662742614,val_loss4.610987784266114\n",
      "2023-01-27 21:13:44.676196Epoch:230,Traing_loss:4.615445980072021,val_loss4.610987784266114\n",
      "2023-01-27 21:22:28.136343Epoch:240,Traing_loss:4.615540159225464,val_loss4.610987784266114\n",
      "2023-01-27 21:31:12.378532Epoch:250,Traing_loss:4.615675468444824,val_loss4.610987784266114\n",
      "2023-01-27 21:39:55.502383Epoch:260,Traing_loss:4.615526668548584,val_loss4.610987784266114\n",
      "2023-01-27 21:48:40.482713Epoch:270,Traing_loss:4.615576525688171,val_loss4.610987784266114\n",
      "2023-01-27 21:57:24.437847Epoch:280,Traing_loss:4.615385618209839,val_loss4.610987784266114\n",
      "2023-01-27 22:06:06.550453Epoch:290,Traing_loss:4.615838249206543,val_loss4.610987784266114\n",
      "2023-01-27 22:14:49.196978Epoch:300,Traing_loss:4.616297607421875,val_loss4.610987784266114\n"
     ]
    }
   ],
   "source": [
    "model,loss_train_lists, loss_val_lists,train_accuracies,val_accuracies = train_loop(n_epochs = 300,\n",
    "                                            optimizer=optimizer,\n",
    "                                            model = model,\n",
    "                                            loss_fn = loss_fn,\n",
    "                                            train = train_loader,\n",
    "                                            val = val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382fec7-e646-4e22-9ad7-5badb35a17a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
